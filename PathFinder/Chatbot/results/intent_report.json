{
  "deny": {
    "precision": 0.9444444444444444,
    "recall": 0.9444444444444444,
    "f1-score": 0.9444444444444444,
    "support": 18,
    "confused_with": {
      "mood_unhappy": 1
    }
  },
  "inform_grade": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 36,
    "confused_with": {}
  },
  "affirm": {
    "precision": 0.95,
    "recall": 0.8636363636363636,
    "f1-score": 0.9047619047619048,
    "support": 22,
    "confused_with": {
      "mood_unhappy": 1,
      "inform_school_system": 1
    }
  },
  "inform_preference": {
    "precision": 1.0,
    "recall": 0.8333333333333334,
    "f1-score": 0.9090909090909091,
    "support": 12,
    "confused_with": {
      "faq": 2
    }
  },
  "faq": {
    "precision": 0.9909297052154195,
    "recall": 1.0,
    "f1-score": 0.9954441913439636,
    "support": 437,
    "confused_with": {}
  },
  "mood_great": {
    "precision": 0.8571428571428571,
    "recall": 0.9230769230769231,
    "f1-score": 0.888888888888889,
    "support": 13,
    "confused_with": {
      "mood_unhappy": 1
    }
  },
  "recommend_college": {
    "precision": 1.0,
    "recall": 0.8823529411764706,
    "f1-score": 0.9375,
    "support": 17,
    "confused_with": {
      "faq": 2
    }
  },
  "inform_school_system": {
    "precision": 0.92,
    "recall": 0.9583333333333334,
    "f1-score": 0.9387755102040817,
    "support": 24,
    "confused_with": {
      "goodbye": 1
    }
  },
  "mood_unhappy": {
    "precision": 0.75,
    "recall": 0.8181818181818182,
    "f1-score": 0.7826086956521738,
    "support": 11,
    "confused_with": {
      "mood_great": 2
    }
  },
  "greet": {
    "precision": 0.8,
    "recall": 0.9230769230769231,
    "f1-score": 0.8571428571428571,
    "support": 13,
    "confused_with": {
      "affirm": 1
    }
  },
  "inform_location": {
    "precision": 1.0,
    "recall": 0.9705882352941176,
    "f1-score": 0.9850746268656716,
    "support": 34,
    "confused_with": {
      "inform_school_system": 1
    }
  },
  "goodbye": {
    "precision": 0.9333333333333333,
    "recall": 0.8235294117647058,
    "f1-score": 0.8749999999999999,
    "support": 17,
    "confused_with": {
      "greet": 2,
      "deny": 1
    }
  },
  "accuracy": 0.9740061162079511,
  "macro avg": {
    "precision": 0.9288208616780046,
    "recall": 0.9117128106098695,
    "f1-score": 0.918227669032908,
    "support": 654
  },
  "weighted avg": {
    "precision": 0.9750394571692081,
    "recall": 0.9740061162079511,
    "f1-score": 0.9739537038839179,
    "support": 654
  },
  "micro avg": {
    "precision": 0.9740061162079511,
    "recall": 0.9740061162079511,
    "f1-score": 0.9740061162079511,
    "support": 654
  }
}